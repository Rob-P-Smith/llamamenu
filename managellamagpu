#!/bin/bash

# Llama.cpp Management Script - GPU/Vulkan Edition
# Default paths - modify as needed
LLAMA_PATH="$HOME/Downloads/llama.cpp/build"
DEFAULT_MODELS_DIR="$HOME/Models"
MODELS_DIR="$DEFAULT_MODELS_DIR"
CONFIG_FILE="$HOME/.managellama.conf"
SERVICE_PID_FILE="/tmp/llama-server.pid"
SERVICE_LOG_FILE="/tmp/llama-server.log"

# Colors for better UI
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
MAGENTA='\033[0;35m'
NC='\033[0m' # No Color
BOLD='\033[1m'

# Load config if exists
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
fi

# Save config
save_config() {
    echo "MODELS_DIR=\"$MODELS_DIR\"" > "$CONFIG_FILE"
    echo "LLAMA_PATH=\"$LLAMA_PATH\"" >> "$CONFIG_FILE"
}

# Generate human-friendly model name
get_friendly_name() {
    local filepath="$1"
    local basename=$(basename "$filepath")

    # Get first word before first dash
    local first_word=$(echo "$basename" | cut -d'-' -f1)

    # Find pattern like "109B" or "7B" etc
    local size_pattern=$(echo "$basename" | grep -o '[0-9]\+B' | head -1)

    if [ -z "$size_pattern" ]; then
        # If no B pattern found, just return the first word
        echo "$first_word"
    else
        # Capitalize first letter of first word
        first_word="$(echo ${first_word:0:1} | tr '[:lower:]' '[:upper:]')${first_word:1}"
        echo "$first_word $size_pattern"
    fi
}

# Get GPU info
get_gpu_info() {
    local gpu_info=""

    # Check for Vulkan
    if command -v vulkaninfo &> /dev/null; then
        local vulkan_device=$(vulkaninfo 2>/dev/null | grep "deviceName" | head -1 | cut -d'=' -f2 | xargs)
        if [ ! -z "$vulkan_device" ]; then
            gpu_info="${GREEN}Vulkan: ${vulkan_device}${NC}"
        fi
    fi

    # If no Vulkan info, check PCI devices
    if [ -z "$gpu_info" ]; then
        local pci_gpu=$(lspci | grep -iE "vga|3d|display" | head -1)
        if [ ! -z "$pci_gpu" ]; then
            gpu_info="${YELLOW}GPU: $(echo $pci_gpu | cut -d':' -f3)${NC}"
        else
            gpu_info="${RED}No GPU detected${NC}"
        fi
    fi

    echo "$gpu_info"
}

# Clear screen and show header
show_header() {
    clear
    echo -e "${CYAN}${BOLD}╔══════════════════════════════════════════════════════════╗${NC}"
    echo -e "${CYAN}${BOLD}║       LLAMA.CPP MANAGEMENT SYSTEM v2.0 (GPU)            ║${NC}"
    echo -e "${CYAN}${BOLD}╚══════════════════════════════════════════════════════════╝${NC}"
    echo -e "${GREEN}Models Directory: ${NC}$MODELS_DIR"
    echo -e "${GREEN}Llama.cpp Path: ${NC}$LLAMA_PATH"
    echo -e "${GREEN}GPU Device: ${NC}$(get_gpu_info)"

    # Check if server is running
    if [ -f "$SERVICE_PID_FILE" ] && kill -0 $(cat "$SERVICE_PID_FILE") 2>/dev/null; then
        echo -e "${GREEN}Server Status: ${NC}${GREEN}● Running (PID: $(cat $SERVICE_PID_FILE))${NC}"
        # Show GPU layers if config exists
        if [ -f /tmp/llama-server-config.sh ]; then
            source /tmp/llama-server-config.sh
            if [ ! -z "$GPU_LAYERS" ]; then
                echo -e "${GREEN}GPU Layers: ${NC}${MAGENTA}$GPU_LAYERS layers offloaded${NC}"
            fi
        fi
    else
        echo -e "${GREEN}Server Status: ${NC}${RED}● Stopped${NC}"
    fi
    echo -e "${CYAN}═══════════════════════════════════════════════════════════${NC}\n"
}

# Main menu
show_menu() {
    echo -e "${YELLOW}${BOLD}Main Menu:${NC}"
    echo -e "${BLUE}  1)${NC} View all models in models directory"
    echo -e "${BLUE}  2)${NC} Change models directory"
    echo -e "${BLUE}  3)${NC} Download new model"
    echo -e "${BLUE}  4)${NC} Start llama server (interactive setup)"
    echo -e "${BLUE}  5)${NC} View server stats (live)"
    echo -e "${BLUE}  6)${NC} Watch server logs"
    echo -e "${BLUE}  7)${NC} Stop llama server"
    echo -e "${BLUE}  8)${NC} Restart llama server"
    echo -e "${BLUE}  9)${NC} Test model (interactive chat)"
    echo -e "${BLUE} 10)${NC} Benchmark model"
    echo -e "${BLUE} 11)${NC} Convert model format"
    echo -e "${BLUE} 12)${NC} System info & GPU details"
    echo -e "${BLUE} 13)${NC} Update llama.cpp"
    echo -e "${BLUE} 14)${NC} List available devices"
    echo -e "${RED}  0)${NC} Exit"
    echo -e "${CYAN}═══════════════════════════════════════════════════════════${NC}"
    echo -n -e "${GREEN}Enter choice [0-14]: ${NC}"
}

# Press any key to continue
press_any_key() {
    echo -e "\n${YELLOW}Press any key to return to main menu...${NC}"
    read -n 1 -s -r
}

# View models
view_models() {
    show_header
    echo -e "${YELLOW}${BOLD}Models in $MODELS_DIR:${NC}\n"

    if [ ! -d "$MODELS_DIR" ]; then
        echo -e "${RED}Models directory does not exist!${NC}"
        press_any_key
        return
    fi

    local count=0
    while IFS= read -r model; do
        ((count++))
        size=$(du -h "$model" | cut -f1)
        basename=$(basename "$model")
        friendly_name=$(get_friendly_name "$model")
        echo -e "${BLUE}$count)${NC} ${BOLD}$friendly_name${NC}"
        echo -e "    File: $basename ${CYAN}[$size]${NC}"
    done < <(find "$MODELS_DIR" -name "*.gguf" -type f 2>/dev/null | sort)

    if [ $count -eq 0 ]; then
        echo -e "${RED}No GGUF models found in $MODELS_DIR${NC}"
    else
        echo -e "\n${GREEN}Total: $count model(s)${NC}"
    fi

    press_any_key
}

# Change models directory
change_models_dir() {
    show_header
    echo -e "${YELLOW}${BOLD}Change Models Directory${NC}\n"
    echo -e "Current directory: ${GREEN}$MODELS_DIR${NC}\n"
    echo -n -e "Enter new models directory path (or 'cancel' to abort): "
    read new_dir

    if [ "$new_dir" = "cancel" ]; then
        echo -e "${YELLOW}Cancelled.${NC}"
        press_any_key
        return
    fi

    # Expand tilde
    new_dir="${new_dir/#\~/$HOME}"

    if [ ! -d "$new_dir" ]; then
        echo -n -e "${YELLOW}Directory doesn't exist. Create it? (y/n): ${NC}"
        read -n 1 create
        echo
        if [ "$create" = "y" ] || [ "$create" = "Y" ]; then
            mkdir -p "$new_dir"
            echo -e "${GREEN}Directory created successfully!${NC}"
        else
            echo -e "${RED}Directory not created. Keeping current setting.${NC}"
            press_any_key
            return
        fi
    fi

    MODELS_DIR="$new_dir"
    save_config
    echo -e "${GREEN}Models directory updated to: $MODELS_DIR${NC}"
    press_any_key
}

# Download model
download_model() {
    show_header
    echo -e "${YELLOW}${BOLD}Download New Model${NC}\n"
    echo -e "Enter the URL of the GGUF model to download"
    echo -e "Example: https://huggingface.co/user/model/resolve/main/model.gguf\n"
    echo -n -e "URL (or 'cancel' to abort): "
    read url

    if [ "$url" = "cancel" ]; then
        echo -e "${YELLOW}Cancelled.${NC}"
        press_any_key
        return
    fi

    # Extract filename from URL
    filename=$(basename "$url" | sed 's/?.*//g')

    echo -n -e "Save as (default: $filename): "
    read custom_name

    if [ ! -z "$custom_name" ]; then
        filename="$custom_name"
    fi

    # Ensure .gguf extension
    if [[ ! "$filename" == *.gguf ]]; then
        filename="${filename}.gguf"
    fi

    target_file="$MODELS_DIR/$filename"

    echo -e "\n${CYAN}Downloading to: $target_file${NC}\n"

    # Download with progress bar
    wget --show-progress -O "$target_file" "$url"

    if [ $? -eq 0 ]; then
        echo -e "\n${GREEN}Download completed successfully!${NC}"
    else
        echo -e "\n${RED}Download failed!${NC}"
        rm -f "$target_file" 2>/dev/null
    fi

    press_any_key
}

# Start server with GPU support
start_server() {
    show_header
    echo -e "${YELLOW}${BOLD}Start Llama Server with GPU Acceleration${NC}\n"

    # Check if already running
    if [ -f "$SERVICE_PID_FILE" ] && kill -0 $(cat "$SERVICE_PID_FILE") 2>/dev/null; then
        echo -e "${RED}Server is already running! (PID: $(cat $SERVICE_PID_FILE))${NC}"
        echo -e "Stop it first before starting a new instance."
        press_any_key
        return
    fi

    # List available models
    echo -e "${CYAN}Available models:${NC}\n"

    models=()
    while IFS= read -r model; do
        models+=("$model")
    done < <(find "$MODELS_DIR" -name "*.gguf" -type f 2>/dev/null | sort)

    if [ ${#models[@]} -eq 0 ]; then
        echo -e "${RED}No models found! Download a model first.${NC}"
        press_any_key
        return
    fi

    for i in "${!models[@]}"; do
        friendly_name=$(get_friendly_name "${models[$i]}")
        basename=$(basename "${models[$i]}")
        echo -e "${BLUE}$((i+1)))${NC} ${BOLD}$friendly_name${NC}"
        echo -e "    $basename"
    done

    echo -n -e "\n${GREEN}Select model number: ${NC}"
    read model_choice

    if ! [[ "$model_choice" =~ ^[0-9]+$ ]] || [ "$model_choice" -lt 1 ] || [ "$model_choice" -gt ${#models[@]} ]; then
        echo -e "${RED}Invalid selection!${NC}"
        press_any_key
        return
    fi

    selected_model="${models[$((model_choice-1))]}"
    selected_model_name=$(get_friendly_name "$selected_model")

    # GPU Configuration
    echo -e "\n${MAGENTA}${BOLD}GPU Configuration:${NC}"

    echo -n -e "Number of layers to offload to GPU (default: 999 for full offload): "
    read gpu_layers
    gpu_layers=${gpu_layers:-999}

    echo -n -e "Split mode (none/layer/row, default: layer): "
    read split_mode
    split_mode=${split_mode:-layer}

    echo -n -e "Main GPU index (default: 0): "
    read main_gpu
    main_gpu=${main_gpu:-0}

    echo -n -e "Disable KV offload? (y/N): "
    read -n 1 no_kv_offload
    echo

    # Get parameters
    cpu_threads=$(nproc)
    echo -e "\n${CYAN}Server Configuration:${NC}"

    echo -n -e "Number of CPU threads (default: $cpu_threads): "
    read threads
    threads=${threads:-$cpu_threads}

    echo -n -e "Context size (default: 16384): "
    read context
    context=${context:-16384}

    echo -n -e "Batch size (default: 2048): "
    read batch
    batch=${batch:-2048}

    echo -n -e "Port (default: 8080): "
    read port
    port=${port:-8080}

    echo -n -e "Host (default: 0.0.0.0): "
    read host
    host=${host:-0.0.0.0}

    echo -n -e "Max parallel requests (default: 4): "
    read parallel
    parallel=${parallel:-4}

    # Add model alias for friendly name
    echo -n -e "Model alias/name (default: $selected_model_name): "
    read model_alias
    model_alias=${model_alias:-$selected_model_name}

    # Build command
    server_cmd="./bin/llama-server \
        -m \"$selected_model\" \
        -a \"$model_alias\" \
        -t $threads \
        -c $context \
        -b $batch \
        --host $host \
        --port $port \
        --parallel $parallel \
        -ngl $gpu_layers \
        --split-mode $split_mode \
        --main-gpu $main_gpu"

    # Add no-kv-offload if requested
    if [ "$no_kv_offload" = "y" ] || [ "$no_kv_offload" = "Y" ]; then
        server_cmd="$server_cmd --no-kv-offload"
    fi

    # Start server in background
    echo -e "\n${CYAN}Starting server with model: ${BOLD}$model_alias${NC}"
    echo -e "${MAGENTA}GPU Layers: $gpu_layers | Split Mode: $split_mode${NC}"

    cd "$LLAMA_PATH"
    eval "nohup $server_cmd > \"$SERVICE_LOG_FILE\" 2>&1 &"

    server_pid=$!
    echo $server_pid > "$SERVICE_PID_FILE"

    # Save last config for restart
    cat > /tmp/llama-server-config.sh << EOF
MODEL="$selected_model"
MODEL_ALIAS="$model_alias"
THREADS=$threads
CONTEXT=$context
BATCH=$batch
HOST=$host
PORT=$port
PARALLEL=$parallel
GPU_LAYERS=$gpu_layers
SPLIT_MODE=$split_mode
MAIN_GPU=$main_gpu
NO_KV_OFFLOAD="$no_kv_offload"
EOF

    sleep 3

    if kill -0 $server_pid 2>/dev/null; then
        echo -e "${GREEN}Server started successfully!${NC}"
        echo -e "PID: $server_pid"
        echo -e "URL: http://$host:$port"
        echo -e "Model: $model_alias"
        echo -e "GPU Layers Offloaded: $gpu_layers"

        # Check if Vulkan was initialized
        if grep -q "ggml_vulkan:" "$SERVICE_LOG_FILE" 2>/dev/null; then
            echo -e "${GREEN}Vulkan GPU acceleration active!${NC}"
            grep "ggml_vulkan:" "$SERVICE_LOG_FILE" | head -2
        fi
    else
        echo -e "${RED}Failed to start server! Check logs.${NC}"
        rm -f "$SERVICE_PID_FILE"
        echo -e "\nLast 10 lines of log:"
        tail -10 "$SERVICE_LOG_FILE"
    fi

    press_any_key
}

# View server stats with GPU monitoring
view_stats() {
    show_header
    echo -e "${YELLOW}${BOLD}Server Statistics (Press Ctrl+C to exit)${NC}\n"

    if [ ! -f "$SERVICE_PID_FILE" ] || ! kill -0 $(cat "$SERVICE_PID_FILE") 2>/dev/null; then
        echo -e "${RED}Server is not running!${NC}"
        press_any_key
        return
    fi

    # Get server config
    if [ -f /tmp/llama-server-config.sh ]; then
        source /tmp/llama-server-config.sh
        server_url="http://$HOST:$PORT"
    else
        server_url="http://0.0.0.0:8080"
    fi

    echo -e "${CYAN}Fetching stats from $server_url/metrics${NC}\n"
    echo -e "Press Ctrl+C to return to menu\n"

    # Monitor stats
    while true; do
        clear
        echo -e "${CYAN}${BOLD}Server Statistics - $(date)${NC}\n"

        # Server config info
        if [ -f /tmp/llama-server-config.sh ]; then
            source /tmp/llama-server-config.sh
            echo -e "${MAGENTA}Model: $MODEL_ALIAS | GPU Layers: $GPU_LAYERS | Context: $CONTEXT${NC}\n"
        fi

        # Try to get metrics
        if command -v curl &> /dev/null; then
            echo -e "${YELLOW}Performance Metrics:${NC}"
            curl -s "$server_url/metrics" 2>/dev/null | grep -E "llama_|process_" | head -20

            echo -e "\n${YELLOW}Health Status:${NC}"
            curl -s "$server_url/health" 2>/dev/null | python3 -m json.tool 2>/dev/null || echo "Unable to fetch health"

            echo -e "\n${YELLOW}Slots Status:${NC}"
            curl -s "$server_url/slots" 2>/dev/null | python3 -m json.tool 2>/dev/null | head -30
        else
            echo -e "${RED}curl not installed. Install it to view stats.${NC}"
        fi

        # GPU Memory monitoring (if available)
        echo -e "\n${YELLOW}GPU Status:${NC}"
        if command -v radeontop &> /dev/null; then
            timeout 1 radeontop -d - -l 1 2>/dev/null | grep -E "gpu|vram|mclk|sclk" | head -5
        else
            echo "Install radeontop for AMD GPU monitoring"
        fi

        sleep 2
    done
}

# Watch logs
watch_logs() {
    show_header
    echo -e "${YELLOW}${BOLD}Server Logs (Press Ctrl+C to exit)${NC}\n"

    if [ ! -f "$SERVICE_LOG_FILE" ]; then
        echo -e "${RED}No log file found!${NC}"
        press_any_key
        return
    fi

    echo -e "${CYAN}Tailing $SERVICE_LOG_FILE${NC}\n"
    echo -e "Press Ctrl+C to return to menu\n"

    # Show GPU initialization info if present
    if grep -q "ggml_vulkan:" "$SERVICE_LOG_FILE" 2>/dev/null; then
        echo -e "${MAGENTA}GPU Initialization:${NC}"
        grep "ggml_vulkan:" "$SERVICE_LOG_FILE" | head -5
        echo -e "${CYAN}═══════════════════════════════════════════════════════════${NC}\n"
    fi

    tail -f "$SERVICE_LOG_FILE"
}

# Stop server
stop_server() {
    show_header
    echo -e "${YELLOW}${BOLD}Stop Llama Server${NC}\n"

    if [ ! -f "$SERVICE_PID_FILE" ]; then
        echo -e "${RED}No PID file found. Server might not be running.${NC}"
        press_any_key
        return
    fi

    pid=$(cat "$SERVICE_PID_FILE")

    if kill -0 $pid 2>/dev/null; then
        echo -e "Stopping server (PID: $pid)..."
        kill $pid
        sleep 2

        if kill -0 $pid 2>/dev/null; then
            echo -e "${YELLOW}Server didn't stop gracefully. Force killing...${NC}"
            kill -9 $pid
        fi

        rm -f "$SERVICE_PID_FILE"
        echo -e "${GREEN}Server stopped successfully!${NC}"
    else
        echo -e "${YELLOW}Server process not found. Cleaning up PID file.${NC}"
        rm -f "$SERVICE_PID_FILE"
    fi

    press_any_key
}

# Restart server with GPU support
restart_server() {
    show_header
    echo -e "${YELLOW}${BOLD}Restart Llama Server${NC}\n"

    if [ ! -f /tmp/llama-server-config.sh ]; then
        echo -e "${RED}No previous server configuration found!${NC}"
        echo -e "Please start the server first using option 4."
        press_any_key
        return
    fi

    # Stop if running
    if [ -f "$SERVICE_PID_FILE" ] && kill -0 $(cat "$SERVICE_PID_FILE") 2>/dev/null; then
        echo -e "Stopping current server..."
        pid=$(cat "$SERVICE_PID_FILE")
        kill $pid 2>/dev/null
        sleep 2
        kill -9 $pid 2>/dev/null
        rm -f "$SERVICE_PID_FILE"
    fi

    # Load previous config
    source /tmp/llama-server-config.sh

    echo -e "Restarting with previous configuration..."
    echo -e "Model: $MODEL_ALIAS"
    echo -e "GPU Layers: $GPU_LAYERS | Split Mode: $SPLIT_MODE"
    echo -e "Threads: $THREADS, Context: $CONTEXT, Batch: $BATCH"
    echo -e "Host: $HOST:$PORT\n"

    # Build command
    server_cmd="./bin/llama-server \
        -m \"$MODEL\" \
        -a \"$MODEL_ALIAS\" \
        -t $THREADS \
        -c $CONTEXT \
        -b $BATCH \
        --host $HOST \
        --port $PORT \
        --parallel $PARALLEL \
        -ngl $GPU_LAYERS \
        --split-mode $SPLIT_MODE \
        --main-gpu $MAIN_GPU"

    if [ "$NO_KV_OFFLOAD" = "y" ] || [ "$NO_KV_OFFLOAD" = "Y" ]; then
        server_cmd="$server_cmd --no-kv-offload"
    fi

    cd "$LLAMA_PATH"
    eval "nohup $server_cmd > \"$SERVICE_LOG_FILE\" 2>&1 &"

    server_pid=$!
    echo $server_pid > "$SERVICE_PID_FILE"

    sleep 3

    if kill -0 $server_pid 2>/dev/null; then
        echo -e "${GREEN}Server restarted successfully!${NC}"
        echo -e "PID: $server_pid"
    else
        echo -e "${RED}Failed to restart server!${NC}"
        rm -f "$SERVICE_PID_FILE"
    fi

    press_any_key
}

# Test model with interactive chat (GPU accelerated)
test_model() {
    show_header
    echo -e "${YELLOW}${BOLD}Test Model (Interactive Chat with GPU)${NC}\n"

    # List models
    models=()
    while IFS= read -r model; do
        models+=("$model")
    done < <(find "$MODELS_DIR" -name "*.gguf" -type f 2>/dev/null | sort)

    if [ ${#models[@]} -eq 0 ]; then
        echo -e "${RED}No models found!${NC}"
        press_any_key
        return
    fi

    for i in "${!models[@]}"; do
        friendly_name=$(get_friendly_name "${models[$i]}")
        basename=$(basename "${models[$i]}")
        echo -e "${BLUE}$((i+1)))${NC} ${BOLD}$friendly_name${NC}"
        echo -e "    $basename"
    done

    echo -n -e "\n${GREEN}Select model number: ${NC}"
    read model_choice

    if ! [[ "$model_choice" =~ ^[0-9]+$ ]] || [ "$model_choice" -lt 1 ] || [ "$model_choice" -gt ${#models[@]} ]; then
        echo -e "${RED}Invalid selection!${NC}"
        press_any_key
        return
    fi

    selected_model="${models[$((model_choice-1))]}"
    selected_model_name=$(get_friendly_name "$selected_model")

    echo -n -e "\n${MAGENTA}Number of GPU layers to offload (default: 999): ${NC}"
    read gpu_layers
    gpu_layers=${gpu_layers:-999}

    echo -e "\n${CYAN}Starting interactive chat with ${BOLD}$selected_model_name${NC}..."
    echo -e "${MAGENTA}GPU Layers: $gpu_layers${NC}"
    echo -e "Type 'exit' or press Ctrl+C to quit\n"

    cd "$LLAMA_PATH"
    ./bin/llama-cli -m "$selected_model" \
        -i \
        --interactive-first \
        --color \
        -c 4096 \
        -t $(nproc) \
        -ngl $gpu_layers \
        --temp 0.7 \
        --repeat-penalty 1.1 \
        -r "User:" \
        --in-prefix " " \
        --in-suffix "Assistant:"

    press_any_key
}

# Benchmark model with GPU
benchmark_model() {
    show_header
    echo -e "${YELLOW}${BOLD}Benchmark Model (GPU Accelerated)${NC}\n"

    models=()
    while IFS= read -r model; do
        models+=("$model")
    done < <(find "$MODELS_DIR" -name "*.gguf" -type f 2>/dev/null | sort)

    if [ ${#models[@]} -eq 0 ]; then
        echo -e "${RED}No models found!${NC}"
        press_any_key
        return
    fi

    for i in "${!models[@]}"; do
        friendly_name=$(get_friendly_name "${models[$i]}")
        basename=$(basename "${models[$i]}")
        echo -e "${BLUE}$((i+1)))${NC} ${BOLD}$friendly_name${NC}"
        echo -e "    $basename"
    done

    echo -n -e "\n${GREEN}Select model number: ${NC}"
    read model_choice

    if ! [[ "$model_choice" =~ ^[0-9]+$ ]] || [ "$model_choice" -lt 1 ] || [ "$model_choice" -gt ${#models[@]} ]; then
        echo -e "${RED}Invalid selection!${NC}"
        press_any_key
        return
    fi

    selected_model="${models[$((model_choice-1))]}"
    selected_model_name=$(get_friendly_name "$selected_model")

    echo -n -e "\n${MAGENTA}Number of GPU layers to offload (default: 999): ${NC}"
    read gpu_layers
    gpu_layers=${gpu_layers:-999}

    echo -e "\n${CYAN}Running benchmark for ${BOLD}$selected_model_name${NC}...${NC}"
    echo -e "${MAGENTA}GPU Layers: $gpu_layers${NC}\n"

    cd "$LLAMA_PATH"
    ./bin/llama-bench -m "$selected_model" -t $(nproc) -ngl $gpu_layers

    press_any_key
}

# Convert model
convert_model() {
    show_header
    echo -e "${YELLOW}${BOLD}Convert Model Format${NC}\n"

    if [ ! -f "$LLAMA_PATH/../convert_hf_to_gguf.py" ]; then
        echo -e "${RED}Conversion script not found!${NC}"
        echo -e "Expected at: $LLAMA_PATH/../convert_hf_to_gguf.py"
        press_any_key
        return
    fi

    echo -e "${CYAN}This will convert HuggingFace models to GGUF format${NC}\n"
    echo -n -e "Enter path to HuggingFace model directory: "
    read hf_path

    if [ ! -d "$hf_path" ]; then
        echo -e "${RED}Directory not found!${NC}"
        press_any_key
        return
    fi

    output_name=$(basename "$hf_path").gguf
    echo -n -e "Output filename (default: $output_name): "
    read custom_output

    if [ ! -z "$custom_output" ]; then
        output_name="$custom_output"
    fi

    echo -e "\n${CYAN}Converting...${NC}"

    cd "$LLAMA_PATH/.."
    python3 convert_hf_to_gguf.py "$hf_path" --outfile "$MODELS_DIR/$output_name"

    if [ $? -eq 0 ]; then
        echo -e "${GREEN}Conversion completed!${NC}"
    else
        echo -e "${RED}Conversion failed!${NC}"
    fi

    press_any_key
}

# Enhanced system info with GPU details
system_info() {
    show_header
    echo -e "${YELLOW}${BOLD}System Information & GPU Details${NC}\n"

    echo -e "${CYAN}CPU Information:${NC}"
    lscpu | grep -E "Model name:|CPU\(s\):|Thread|Core|Socket" | sed 's/^/  /'

    echo -e "\n${CYAN}Memory Information:${NC}"
    free -h | sed 's/^/  /'

    echo -e "\n${MAGENTA}GPU Information (Vulkan):${NC}"
    if command -v vulkaninfo &> /dev/null; then
        vulkaninfo 2>/dev/null | grep -E "deviceName|deviceType|apiVersion|driverVersion" | head -8 | sed 's/^/  /'
        echo -e "\n  ${YELLOW}Vulkan Memory:${NC}"
        vulkaninfo 2>/dev/null | grep -E "size|heap" | head -6 | sed 's/^/    /'
    else
        echo "  Vulkaninfo not found"
    fi

    echo -e "\n${CYAN}PCI GPU Devices:${NC}"
    lspci | grep -iE "vga|3d|display" | sed 's/^/  /'

    echo -e "\n${CYAN}Disk Usage (Models Directory):${NC}"
    df -h "$MODELS_DIR" | sed 's/^/  /'

    echo -e "\n${CYAN}Llama.cpp Build Info:${NC}"
    if [ -f "$LLAMA_PATH/bin/llama-cli" ]; then
        echo "  Binary: llama-cli"
        ls -lh "$LLAMA_PATH/bin/llama-cli" | awk '{print "  Size: " $5 " | Modified: " $6 " " $7 " " $8}'

        # Check for Vulkan support
        if [ -f "$LLAMA_PATH/bin/libggml-vulkan.so" ]; then
            echo -e "  ${GREEN}Vulkan Support: ENABLED${NC}"
            ls -lh "$LLAMA_PATH/bin/libggml-vulkan.so" | awk '{print "  Vulkan Library Size: " $5}'
        else
            echo -e "  ${RED}Vulkan Support: NOT FOUND${NC}"
        fi
    else
        echo "  Not found"
    fi

    # AMD GPU specific tools
    echo -e "\n${CYAN}AMD GPU Tools:${NC}"
    if command -v radeontop &> /dev/null; then
        echo -e "  ${GREEN}radeontop: Available${NC}"
    else
        echo -e "  ${YELLOW}radeontop: Not installed (install for GPU monitoring)${NC}"
    fi

    if command -v rocm-smi &> /dev/null; then
        echo -e "  ${GREEN}rocm-smi: Available${NC}"
        rocm-smi --showproductname 2>/dev/null | head -5 | sed 's/^/    /'
    else
        echo -e "  ${YELLOW}ROCm: Not installed${NC}"
    fi

    press_any_key
}

# Update llama.cpp
update_llama() {
    show_header
    echo -e "${YELLOW}${BOLD}Update Llama.cpp${NC}\n"

    llama_root=$(dirname "$LLAMA_PATH")

    if [ ! -d "$llama_root/.git" ]; then
        echo -e "${RED}Not a git repository!${NC}"
        echo -e "Cannot update automatically."
        press_any_key
        return
    fi

    echo -e "${CYAN}Current directory: $llama_root${NC}\n"

    cd "$llama_root"

    echo -e "Fetching updates..."
    git fetch

    echo -e "\n${CYAN}Current version:${NC}"
    git log --oneline -1

    echo -e "\n${CYAN}Latest version:${NC}"
    git log --oneline origin/master -1

    echo -n -e "\n${YELLOW}Update to latest version? (y/n): ${NC}"
    read -n 1 update
    echo

    if [ "$update" != "y" ] && [ "$update" != "Y" ]; then
        echo -e "${YELLOW}Update cancelled.${NC}"
        press_any_key
        return
    fi

    echo -e "\n${CYAN}Updating...${NC}"
    git pull origin master

    echo -e "\n${CYAN}Rebuilding with Vulkan support...${NC}"
    cd "$LLAMA_PATH"
    cmake --build . -j$(nproc)

    if [ $? -eq 0 ]; then
        echo -e "\n${GREEN}Update completed successfully!${NC}"

        # Verify Vulkan support still present
        if [ -f "$LLAMA_PATH/bin/libggml-vulkan.so" ]; then
            echo -e "${GREEN}Vulkan support verified!${NC}"
        else
            echo -e "${YELLOW}Warning: Vulkan library not found after rebuild${NC}"
        fi
    else
        echo -e "\n${RED}Update failed!${NC}"
    fi

    press_any_key
}

# List available devices
list_devices() {
    show_header
    echo -e "${YELLOW}${BOLD}Available Compute Devices${NC}\n"

    echo -e "${CYAN}Checking available devices...${NC}\n"

    cd "$LLAMA_PATH"
    if [ -f "./bin/llama-cli" ]; then
        ./bin/llama-cli --list-devices 2>&1 | head -20
    else
        echo -e "${RED}llama-cli not found!${NC}"
    fi

    echo -e "\n${MAGENTA}Vulkan Devices:${NC}"
    if command -v vulkaninfo &> /dev/null; then
        vulkaninfo --summary 2>/dev/null | grep -A 5 "Devices:" | sed 's/^/  /'
    else
        echo "  vulkaninfo not available"
    fi

    press_any_key
}

# Main loop
main() {
    while true; do
        show_header
        show_menu
        read choice

        case $choice in
            1) view_models ;;
            2) change_models_dir ;;
            3) download_model ;;
            4) start_server ;;
            5) view_stats ;;
            6) watch_logs ;;
            7) stop_server ;;
            8) restart_server ;;
            9) test_model ;;
            10) benchmark_model ;;
            11) convert_model ;;
            12) system_info ;;
            13) update_llama ;;
            14) list_devices ;;
            0)
                echo -e "\n${GREEN}Goodbye!${NC}"
                exit 0
                ;;
            *)
                echo -e "${RED}Invalid option!${NC}"
                sleep 1
                ;;
        esac
    done
}

# Run main function
main
